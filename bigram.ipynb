{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Language Model\n",
    "\n",
    "## N-grams\n",
    "\n",
    "An n-gram is a contiguous sequence of _n_ tokens (characters, words, or subwords) from a given text.  \n",
    "A 2-gram, or bigram, groups tokens into pairs.\n",
    "\n",
    "For example, using words as tokens, in the sentence:\n",
    "\n",
    "- `Charlie Parker is as good as Bach`\n",
    "\n",
    "The 2-grams would be:\n",
    "\n",
    "- `Charlie Parker`\n",
    "- `Parker is`\n",
    "- `is as`\n",
    "- `as good`\n",
    "- `good as`\n",
    "- `as Bach`\n",
    "\n",
    "> A `BigramLanguageModel` is a statistical model that estimates the probability of a token in a sequence based only on the previous token.\n",
    "\n",
    "### Bigram Probability Function\n",
    "\n",
    "where:\n",
    "\n",
    "- $ P(w*n \\mid w*{n-1}) $ is the **conditional probability** of the word $ w*n $ occurring given that $ w*{n-1} $ came before it.\n",
    "- $ \\text{Count}(w*{n-1}, w_n) $ is the number of times the word pair (bigram) $ (w*{n-1}, w_n) $ appears in the training corpus.\n",
    "- $ \\text{Count}(w*{n-1}) $ is the number of times the word $ w*{n-1} $ appears as a starting word in any bigram.\n",
    "\n",
    "$$ P(w*n | w*{n-1}) = \\frac{\\text{Count}(w*{n-1}, w_n)}{\\text{Count}(w*{n-1})} $$\n",
    "\n",
    "**How does the model know which word comes next?**\n",
    "\n",
    "The model is trained on structured data (our bigrams). It learns that some token pairs are more likely to occur together than others. When it sees a token, it calculates the probability of the next token based on how frequently they appeared together in training data.\n",
    "\n",
    "## Disclaimer\n",
    "\n",
    "Since we want to build an LLM from scratch, our approach is not based on a traditional probability function. Instead, our model uses a neural network with an embedding layer whose weights are updated during each training step, and this is what the following code implements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the env being used\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Just to check this damn library works XD\n",
    "import pylzma\n",
    "# print('pylzma is working')\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# print(f'Device :{device}')\n",
    "\n",
    "## Parameters\n",
    "block_size = 8\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character-based BigramLanguageModel\n",
    "\n",
    "In this example we are going to build a characted-based BLM. Our tokens will be single chars. It is not a great model, but simplifies the example.\n",
    "So, open a file, define a set of tokens (the unique characters appearing in that file) and stablish a vocab_size, which is just the size of this set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open some data\n",
    "with open ('./data/ecce_homo.txt', 'r', encoding='utf-8') as f: \n",
    "    text = f.read()\n",
    "\n",
    "char_set = sorted(set(text))\n",
    "vocab_size = len(char_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding and Decoding\n",
    "\n",
    "Language models operate on numbers, so we need a way to transform characters to numbers and viceversa.\n",
    "Encode recieves a string and returns an array with encoded characters.\n",
    "Decode recieves an array and decodes back each number to the corresponding character in our set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive text-to-int char encoding\n",
    "\n",
    "string_to_int = { ch: i  for i, ch in enumerate(char_set)}\n",
    "int_to_string = { i : ch for i, ch in enumerate(char_set)}\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data\n",
    "\n",
    "The data is split into:\n",
    "\n",
    "- **Train set** → Used for training the model.\n",
    "- **Validation set** → Used to test generalization (data not seen by the model before).\n",
    "\n",
    "#### `get_batch`\n",
    "\n",
    "This function prepares a batch (a unit of training/evaluation data) to fit into the model.\n",
    "\n",
    "- `ix` is a randomly generated sequence of indices referencing starting points in the dataset.\n",
    "- `block_size` defines the window (context size) given to the model to predict the next token.\n",
    "- `x` is a tensor of shape `(batch_size, block_size)`, where:\n",
    "  - Each row is a sequence of `block_size` tokens from `data`, starting at a randomly chosen index.\n",
    "- `y` is a **shifted** version of `x`, meaning:\n",
    "  - Each `y[j]` contains the **next tokens** of `x[j]`.\n",
    "  - This allows the model to learn **the probability of the next token** at each position in `x`.\n",
    "\n",
    "> In the current model, as we'll se, the window is not used for anything.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "\n",
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # stack converts a list of tensors into a tensor itself\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# x, y = get_batch('train')\n",
    "# print('inputs: ')\n",
    "# print(x)\n",
    "# print('targets: ')\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model\n",
    "\n",
    "Thanks to PyTorch, we are not building entirely (or even nearly) from scratch. A `nn.Module` is a _\"Base class for all neural network modules\"_, and offers a set of functionality related to defining layers, managing parameters, moving models between CPU/GPU, etc.\n",
    "\n",
    "Our model initializes with just the vocab_size and creates a token embeding table. This inocent table presented in the course as 'nah, just a lookup table' is a trainable layer of the model. It contains as many rows as `vocab_size`, each row containing `vocab_size` elements and at this point is the layer we are just training: is 'the model' itself.\n",
    "\n",
    "This nn.Embedding layer can accept an input that is a single index, a list of indices, or even a batch of indices, and it will return the corresponding embedding for each index. An embedding will contain a list of `vocab_size` elements representing raw values which later are converted to the probabilities for each token to follow the one provided by our index.\n",
    "\n",
    "#### Input\n",
    "\n",
    "The input is a Tensor of shape `(B, T)`, being `B` the batch size and `T` the sequence length.\n",
    "In the current's model state, the sequence is not really doing anything, since the model is not context aware, so in the `(B, T, C)`, the `T` could be 1 and nothing will change. But, maybe later in the course they introduce some cool features which can use this context.\n",
    "\n",
    "#### Forward function\n",
    "\n",
    "Responsible for calculating a single prediction and (if targets are provided) calculate the loss value throghout the `cross_entropy` function.\n",
    "\n",
    "**Reshaping**\n",
    "Logits have shape `(B, T, C)`, being `C` the vocab size. Due to **_Torch-Magic™_** accepting whatever you throw at it, the call to `self.token_embeding_table` accepts any `(B, T)` Tensor and returns another Tensor with the added embeddings dimension.\n",
    "\n",
    "> `indices = [[0], [0]]` &rarr; `B = 2, T = 1`  \n",
    "> `logits` &rarr; `B = 2, T = 1, C = vocab_size`\n",
    "\n",
    "> This will return a Tensor containing 2 (`B`) sets of 1 (`T`) embeding of dimension `C`\n",
    "\n",
    "The scary dimension-juggling is just about flattening the tensors to adapt them to expected parameters in the cross_entropy function.\n",
    "\n",
    "> `targets = targets.view(B*T)` &rarr; if targets is `[[0, 1], [2, 3]]` is converted to `[0, 1, 2, 3]`  \n",
    "> `logits = logits.view(B*T, C)` &rarr; just flat the embedings, so we get a i \\* j Tensor.\n",
    "\n",
    "#### Generate function\n",
    "\n",
    "This function generates new tokens one at a time, up to `max_new_tokens`, by repeatedly:\n",
    "\n",
    "1. Computing probabilities for the next token\n",
    "\n",
    "1. Sampling a token from those probabilities\n",
    "\n",
    "1. Adding the sampled token to the sequence\n",
    "\n",
    "**Reshape juggling again**  \n",
    "Here `logits = logits[:, -1, :]`, as we are not passing targets to the forward function we recieve a `(B, T, C)` Tensor. Then, for each batch, we take the embedding (the prediction) generated by the last token in the batch sequence.\n",
    "Then softmax transforms the values in the embedding to probabilities and we take one base on a multinomial distribution.\n",
    "Finally we take the generated token and append it (cat &rarr; concat) to the input, so the next generated token in every batch will be based on that last generated token.\n",
    "\n",
    "### Why the hell the model accepts any B\\*T input?\n",
    "\n",
    "Because the model can be trained in parallel. Again, thanks to Torch-Magic™, operations like softmax and multinomial sampling automatically adapt to the input dimensions. This means you can throw various shapes at these functions, and they'll \"just work,\" handling the dimension broadcasting and manipulation behind the scenes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embeding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embeding_table(index)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # predictions\n",
    "            logits, _ = self.forward(index)\n",
    "            # last time step\n",
    "            logits = logits[:, -1, :] # (B, T, C) =>  (B, C)\n",
    "            # softMax\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C), calculates probabilities on the C axis, the embeddings\n",
    "            # sample from distribution\n",
    "            index_next  = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim = 1) # (B, T+1)\n",
    "        return index\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(context)\n",
    "# generated_chars = decode(m.generate(context, max_new_tokens=1)[0].tolist())\n",
    "# print(generated_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 500\n",
    "\n",
    "# Just a function to evaluate the model on the fly, while it is training\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We have a model but is not trained. For this we need to optimize the lost function.\n",
    "Torch exposes a bunch of optimizers better suited for one or another task.\n",
    "\n",
    "To train, just iterate over:\n",
    "\n",
    "- Get some train data (input, expected).\n",
    "- Call model.forward to get the predictions and loss.\n",
    "- Reset gradients to not accumulate them between iterations.\n",
    "- Apply backward propagation.\n",
    "- Update parameters (step).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch optimizer\n",
    "learning_rate = 3e-1\n",
    "dropout = 0.2\n",
    "max_iters = 10000\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for item in range(max_iters):\n",
    "    if item % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {item}\\t | train_loss: {losses['train']:.4f} | val_loss: {losses['val']:.4f}\")\n",
    "    # sample batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict\n",
    "\n",
    "And this is pretty much all, we trained a model based on a newral network with one embedding layer to generate some predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=200)[0].tolist())\n",
    "print(generated_chars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
