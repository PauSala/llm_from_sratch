{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f'Device :{device}')\n",
    "\n",
    "# Parameters\n",
    "block_size = 16\n",
    "batch_size = 32\n",
    "max_iters = 1000\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 100\n",
    "dropout= 0.2\n",
    "n_embd = 100 # why?\n",
    "n_layer = 4\n",
    "n_head = 5\n",
    "\n",
    "# Read data\n",
    "with open ('./data/test.txt', 'r', encoding='utf-8') as f: \n",
    "    text = f.read()\n",
    "\n",
    "char_set = sorted(set(text))\n",
    "vocab_size = len(char_set)\n",
    "\n",
    "\n",
    "# Encoder decoder\n",
    "\n",
    "string_to_int = { ch: i  for i, ch in enumerate(char_set)}\n",
    "int_to_string = { i : ch for i, ch in enumerate(char_set)}\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.int64)\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "\n",
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # stack converts a list of tensors into a tensor itself\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "## Log training function\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learned\n",
    "\n",
    "At the start of this second part of the course, I realized that torch.nn allows modules to be nested. This means you can have a main module with a layer that is itself a module, which can contain layers that are modules as well, and so on. Each module has its own forward function implementation, where its submodules are called to perform computations on the input data.\n",
    "\n",
    "This is why a model can be seen as a large computational graph, where each leaf represents a computation. In fact, PyTorch dynamically builds a directed acyclic graph on the model so all makes sense now.\n",
    "\n",
    "And this means that these things are composable, which is pretty cool, I think, because it allows us to build blocks or pieces of the graph separately and then compose them together to create more complex structures, which is very convinient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model\n",
    "\n",
    "Out GPT-Like mode is just a big computational graph builded from some rehusable modles.\n",
    "\n",
    "This modules are:\n",
    "\n",
    "- Head\n",
    "- MultiHeadAttention\n",
    "- FeedForward\n",
    "- Decoder Block\n",
    "\n",
    "The composition of this modules with other nn.Module predefined layers conform the model, which can be sumarized in this graph:\n",
    "\n",
    "![gpt-like-arch](./gpt-arch.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Head\n",
    "\n",
    "The head receives an input of shape (B, T, C), where:\n",
    "\n",
    "- `B` is the batch size (number of sequences processed at once).\n",
    "- `T` is the sequence length (number of tokens in each sequence).\n",
    "- `C` is the embedding dimension (n_embd).\n",
    "\n",
    "Each attention head has its own set of Query (Q), Key (K), and Value (V) projections, which are implemented as nn.Linear layers. These layers transform the input from (B, T, C) â†’ (B, T, hs), where:\n",
    "\n",
    "- `hs = n_embd // n_head` is the \"head size\" (the portion of the embedding each head attends to).\n",
    "- nn.Linear maps C (context size) to hs (head size), effectively splitting the full embedding across multiple attention heads.\n",
    "- Each head only processes part of the embedding dimensions rather than the entire embedding vector.\n",
    "\n",
    "### Attention mechanism\n",
    "\n",
    "At the forward pass, k, q, v are initialized with x.\n",
    "Somehow, the multiplication (dot product) of q and k (transposed) gives us (after appliing the sqrt normalization trick to avoid gradients exploding) an affinity score.\n",
    "\n",
    "For example:\n",
    "\n",
    "```text\n",
    "q = [\n",
    "     [[1, 0, 1], # Token 1\n",
    "     [0, 1, 1],  # Token 2\n",
    "     [1, 1, 0],  # Token 3\n",
    "     [0, 0, 1]]  # Token 4\n",
    "] # (1, 4, 3)\n",
    "\n",
    "k = [\n",
    "     [[1, 0, 1], # Token 1\n",
    "     [0, 1, 1],  # Token 2\n",
    "     [1, 1, 0],  # Token 3\n",
    "     [0, 0, 1]]  # Token 4\n",
    "] # (1, 4, 3)\n",
    "\n",
    "q @ k.T = [\n",
    "    [2, 1, 1, 1],  # Similarity of Token 1 with all tokens\n",
    "    [1, 2, 1, 1],  # Similarity of Token 2 with all tokens\n",
    "    [1, 1, 2, 0],  # Similarity of Token 3 with all tokens\n",
    "    [1, 1, 0, 1]   # Similarity of Token 4 with all tokens\n",
    "]  # (1, 4, 4)\n",
    "```\n",
    "\n",
    "Another way to understand this calculation is by thinking in terms of cosine similarity. Two vectors pointing in the same direction have an angle \\( \\theta = 0^\\circ \\), so \\( \\cos(0^\\circ) = 1 \\), which means they are perfectly aligned. This is exactly what the dot product calculates: the degree of alignment (or similarity) between vectors. If the vectors are aligned, the dot product is large (and positive); if they are perpendicular, the dot product is zero; and if they point in opposite directions, the dot product is negative.\n",
    "\n",
    "Since we don not want the model to train on future tokens, a tril is aplied, ensuring each token only attends to itself and previous tokens.\n",
    "\n",
    "And this is pretty much all. The rest of the Modules are self-explainatory if you followed the previous notebook (bigram.ipynb). There are lots of things going on, but all of them follow the same principles.\n",
    "\n",
    "### Some clarifications\n",
    "\n",
    "In this line of code, in the MultiHeadAttention module:\n",
    "\n",
    "- `self.heads = nn.ModuleList`\n",
    "\n",
    "The ModuleList is the key to transform parallel heads into just a Tensor, notice this line:\n",
    "\n",
    "- `out = torch.cat([h(x) for h in self.heads], dim=-1)`\n",
    "\n",
    "Which results in a Tensor of shape `(B, T, C) -> (B, T, [h1, h1, h1, h1, h2, h2, h2, h2, h3, h3, h3, h3])`\n",
    "\n",
    "And:\n",
    "\n",
    "- `self.proj = nn.Linear(head_size * num_heads, n_embd)`\n",
    "\n",
    "Notice that the projection layer reshapes again the result of the multiHead computations into the original shape by taking the head_size\\*num_heads as inputs and reshaping to n_embed as outputs.\n",
    "\n",
    "This way we can go from a Tensor `(B, T, hs*num_heads)` comming from all `Head` parallel instances to a `(B, T, C)` again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        _,T,_ = x.shape # B, T, C\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, n_embd):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, F) -> (B, T, [h1, h1, h1, h1, h2, h2, h2, h2, h3, h3, h3, h3])\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x\n",
    "    \n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) # Here the `*` is for unpackaging an iterable into params. \n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape\n",
    "        \n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(index) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            index_cond = index[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, _ = self.forward(index_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(context)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for item in range(max_iters):\n",
    "    if item % eval_iters == 0:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step: {item}\\t | train_loss: {losses['train']:.4f} | val_loss: {losses['val']:.4f}\")\n",
    "    # sample batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=200)[0].tolist())\n",
    "print(generated_chars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
